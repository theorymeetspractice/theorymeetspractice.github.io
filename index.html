<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Theory Meets Practice Workshop 2020</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/light.css"
    />
    <link
      rel="stylesheet"
      href="style.css"
    />
  </head>

  <body>  
    <h1>Advances in ML: Theory Meets Practice</h1>
    <div class="block">    
    <p>
      <strong>13:30-17:00, January 26, 2020.</br>
      Room 4ABC, <a href="https://goo.gl/maps/bXCNLKHwMKeeJchr6">SwissTech Convention Center</a>, Lausanne</strong></br>
      Workshop of the <a href="https://appliedmldays.org/">Applied Machine Learning Days 2020</a>
    </p>
    </div>
    <p>
      The <i>theorey meets practice</i> workshop series aims to bring practitioners and theoreticians together and to stimulate the exchange between experts from industry and academia.
    </p>
    <p>
     For practitioners, the workshop should give an idea of exciting new developments which they can <i>use</i> in their work. For theorists, it should provide a forum to frame the practicality of assumptions and recent work, as well as potentially interesting open questions.
    </p>
    
    <h2>Organizers</h2>
    
    <ul>
      <li><a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/">Aymeric Dieuleveut</a> (Assistant Professor, CMAP, École Polytechnique)</li>    
      <li><a href="https://sstich.ch/">Sebastian Stich</a> (Research Scientist, EPFL)</li>
    </ul>

    <h2>Schedule</h2>
    <table>
      <thead>
        <tr>
          <th style="width:130px">Time</th>
          <th>Speaker</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>13:30</strong>-14:00</td>
          <td><strong>Dan Alistarh</strong></br>
              <a href="#talk1"><i>Compressing Deep Neural Networks for Fun and Profit</i></a>
          </td>
        </tr>
        <tr>
          <td>14:00-14:30</td>
          <td><strong>Nathanael Perraudin</strong> <a href="2020/slides-nathanael.pdf">[Slides]</a></br>
              <a href="#talk4"><i>Deepsphere: an almost equivariant graph-based spherical CNN</i></a>
          </td>
        </tr>
        <tr>
          <td>14:30-14:45</td>
          <td><strong>Short Break</strong> (coffee served on the upper level)
          </td>
        </tr>        
        <tr>
          <td><strong>14:45</strong>-15:15</td>
          <td><strong>Nima Riahi</strong></br>
              <a href="#talk3"><i>Complex Problems in Rail Transport</i></a>
          </td>
        </tr>
        <tr>
          <td>15:15-15:45</td>
          <td><strong>Julie Josse</strong></br>
              <a href="#talk2"><i>A Missing Value Tour</i></a> <a href="2020/slides-julie.pdf">[Slides]</a>
          </td>          
        </tr>
        <tr>
          <td>15:45--16h50</td>
          <td><strong>Hands-on Session: Julie Josse </strong><a href="2020/slides-julie-lab.pdf">[Slides]</a></br>
              <a href="#talk2"><i>Review on Missing Values Methods with Demos</i></a></br>
              Participants should bring their <mark>own laptop</mark>.</br>
              <a href="2020/tmp-lab-missingvalues.zip">Lab materials (zip)</a></br>
              To install R and Rstudio, follow the steps described  <a href="https://thinkr.fr/procedure_en.html">here</a>.</br>
              (you only need to download R and Rstudio, and install both -- you do not need to install git, Rtools, etc.)
          </td>          
        </tr>        
      </tbody>
    </table>

    <h2>Speakers and Talks</h2>
    
    <div class="speaker" id="talk1">
      <div class="avatar">
          <img src="pics/alistarh.jpg" alt="Alistarh">
      </div>    
      <div class="infos">
        <h3><a href="https://people.csail.mit.edu/alistarh/">Dan Alistarh</a></h3>
        <h4>Assistant Professor, IST Austria</br>
            Machine Learning Research Lead at <a href="https://neuralmagic.com/">Neural Magic</a></h4>

        <p><strong>
          Compressing Deep Neural Networks for Fun and Profit
        </strong></p>
        <p>
          Deep learning continues to make significant advances, solving tasks
          from image classification to translation or reinforcement learning. One aspect
          of the field receiving considerable attention is efficiently executing deep
          models efficiently on resource-constrained environments, such as mobile or
          end-user devices. This talk focuses on this question, and will overview some of
          the model compression techniques we have developed over the past couple of
          years, and applied to practice at NeuralMagic, a Boston-based startup. In
          particular, I will talk about tools for inducing high weight (kernel)
          sparsity in convolutional neural networks, as well as techniques for exploiting
          and enhancing activation sparsity in deep networks with ReLU-based networks. The
          lecture will also include a demo, showcasing some of the practical speedups
          we can achieve on real deployments.
        </p>
      </div>
    </div>
    
    <div class="speaker" id="talk2">
      <div class="avatar">
          <img src="pics/josse.png" alt="Josse">
      </div>    
      <div class="infos">
        <h3><a href="http://juliejosse.com/">Julie Josse</a></h3>
        <h4>Professor, CMAP, École Polytechnique</h4>

        <p><strong>
          A Missing Value Tour
        </strong></p>
        <p>
          In many application settings, the data have missing features which make data analysis challenging. An abundant literature addresses missing data as well as more than 150 R packages. In this presentation, I will give an overview on different topics/methods to handle missing values. We will discuss about the inferential framework  where the aim is to estimate at best the parameters and their variance in the presence of missing data, matrix completion methods where the aim is to impute as well as possible and also recent results in a supervised-learning setting.
        </p>
      </div>
    </div>
    
    <div class="speaker" id="talk4">
      <div class="avatar">
          <img src="pics/perraudin.jpg" alt="Perraudin">
      </div>    
      <div class="infos">
        <h3><a href="https://perraudin.info/">Nathanael Perraudin</a></h3>
        <h4>Senior Data Scientist, <a href="https://www.datascience.ch/">Swiss Data Science Center (SDSC)</a></h4>
        
        <p><strong>
          Deepsphere: an almost equivariant graph-based spherical CNN
        </strong></p>
        <p>
          In the last 5 years, the field of Machine Learning has been revolutionized by the success of deep learning. Thanks to the increasing availability of data and computations, we now are able to train very complex/deep models and hence solve challenging tasks better than we ever did.</br>
          Nevertheless, Deep Learning is successful when the network architecture exploits properties of the data, allowing efficient and principled learning. For example, convolutional neural networks (CNNs) revolutionized computer vision because the network architecture has been specifically designed to deal with images. CNNs have important advantages, but their main strength is that they are built for translation equivariance, meaning that if the input is translated, so will the output, which allows spatial weight sharing using few parameters compared to a full input parametrization, and therefore they are computationally efficient.</br>
          Unfortunately, not all datasets are images and we need architectures that adapt to other types of data, encoding both domain specific knowledge and data specific characteristics.
        </p><p>
          In this talk, we will show how graph neural networks can be used to adapt convolution to irregular data domains. In particular we focus on data lying on a sphere and discuss how rotational equivariance can be achieved. Eventually, through the example of the sphere, this talk aims at providing intuitions on how graph neural networks behave in geometrical framework. Hopefully, by the end, you should be able to judge if they have a place in your next deep architecture.</br>
        </p>
      </div>
    </div>

    <div class="speaker" id="talk3">
      <div class="avatar">
          <img src="pics/nima.jpg" alt="avatar">
      </div>        
      <div class="infos">
        <h3><a href="http://riahi.ch/nima/">Nima Riahi</a></h3>
        <h4>Senior Data Scientist, SBB CFF FFS</h4>
        
        <p><strong>
          Complex Problems in Rail Transport
        </strong></p>
        <p>
          The planning, maintenance, and daily operation of a rail system are complex, interrelated problems involving rolling stock, crew, infrastructure, passengers, and other factors. In the past and today, some of this complexity is addressed by conventional analytics or operations research techniques. But new developments in the field of AI and computer hardware allow for new approaches to tackle the hard problems faced by the industry. We will introduce a few problems (from the fields of delay propagation, routing/planning and sensor-based environmental awareness of locomotives) and discuss what currently is or may be attempted to address them.
        </p>
      </div>
    </div>    
    
    
    <h2>Previous Workshops</h2>
    
    <ul>
      <li><a href="https://docs.google.com/document/d/e/2PACX-1vSJz6iSW3XFZHHujPm1C5i9PN1lGH9YnOztvg8c-Bn8iBiGukE2UFHh6S5WoBxXRzYw72WN8XPUge4Z/pub">Program and Speakers 2019</a></li>
      <li><a href="https://docs.google.com/document/d/e/2PACX-1vT7JbYzRxixAaq9asEj1r2IbaLD1y2Fcjvs3EMJ8byS-DJD1U6OUY3zq4imsq8C4n_wwuBvAsom1Rbf/pub">Program and Speakers 2018</a></li>
    </ul>
    
  </body>

</html>
